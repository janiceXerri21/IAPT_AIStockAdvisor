{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for data manipulation\n",
    "import glob # for reading files in a random order\n",
    "import matplotlib.pyplot as plt #for plotting\n",
    "import matplotlib.dates as mdates\n",
    "import talipp as ta\n",
    "from talipp.indicators import ADX\n",
    "import talib as tb\n",
    "import math as mth\n",
    "import numpy as np\n",
    "# importing the random module\n",
    "import random\n",
    "import lightgbm as lgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import gc\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accessing the databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting a CSV files list from a folder\n",
    "file_path1 = './SingleExampleTrain/AAPL.csv'\n",
    "\n",
    "#Reading all CSV files in a list\n",
    "appleDb = pd.read_csv(file_path1)\n",
    "#print(appleDb.to_string()) - to view the original dataframe\n",
    "\n",
    "# Convert Date column to datetime object\n",
    "appleDb['Date'] = pd.to_datetime(appleDb['Date'], utc= True) #set the utc parameter to True to ensure that the timezone information is preserved\n",
    "\n",
    "# Filter rows based on condition - starting from the year 2008\n",
    "appleDb = appleDb[pd.to_datetime(appleDb['Date']).dt.year >= 2008]\n",
    "\n",
    "print(appleDb.to_string())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following stock prices for \n",
    "- AAPL which stands for the Apple Inc. company. \n",
    "- AAL which stands for the American Airlines Group Inc. for Major Airlines\n",
    "- APA which stands for the Apache Corporation for independent oil and gas\n",
    "\n",
    "This dataset contains the following information:\n",
    "\n",
    "Date: This is the date that the stock market was open (i.e. the day that the stock was traded). It's usually listed in a format like \"MM/DD/YYYY\" (month/day/year).\n",
    "\n",
    "Open: This is the price at which the stock opened for trading on that particular day. It's the price that the first trade was made at.\n",
    "\n",
    "High: This is the highest price that the stock reached during trading on that day.\n",
    "\n",
    "Low: This is the lowest price that the stock reached during trading on that day.\n",
    "\n",
    "Close: This is the price at which the stock closed for trading on that particular day. It's the price that the last trade was made at.\n",
    "\n",
    "Volume: This is the total number of shares that were traded on that particular day. It's a measure of how active the market was for that stock on that day.\n",
    "\n",
    "Dividends: These are payments that companies sometimes make to their shareholders as a way of distributing profits. They're usually paid out on a regular basis (e.g. quarterly) and are typically a percentage of the company's earnings.\n",
    "\n",
    "Stock Splits: This is when a company decides to divide its existing shares into multiple shares. For example, if a company had 1 million shares outstanding and decided to do a 2-for-1 stock split, there would then be 2 million shares outstanding, but each share would be worth half as much as before. Companies often do stock splits to make their shares more affordable to individual investors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the prices into logs representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking how all the columns are listed in the dataframe\n",
    "appleDb.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking how all the rows are listed in the dataframe\n",
    "appleDb.index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(appleDb.to_string())\n",
    "\n",
    "\n",
    "def changeValuesIntoLogs(df):\n",
    "    df['Open'] = df['Open'].apply(lambda x: mth.log(x))\n",
    "    df['High'] = df['High'].apply(lambda x: mth.log(x))\n",
    "    df['Low'] = df['Low'].apply(lambda x: mth.log(x))\n",
    "    df['Close'] = df['Close'].apply(lambda x: mth.log(x))\n",
    "    return df\n",
    "\n",
    "appleDb = changeValuesIntoLogs(appleDb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After converting with logs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(appleDb.to_string())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs Representations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Graphs - The opening price of the stock on that day. This helps to identify trends in the stock's price and evaluate whether the stock is undervalued or overvalued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plotting the closing price of the stocks\n",
    "\n",
    "# # create a figure and axis objects with 3 rows and 1 column\n",
    "# fig, axs = plt.subplots(3, 1, figsize=(8, 10))\n",
    "\n",
    "# # plot the open prices for each company on each axis object\n",
    "# axs[0].plot(appleDb['Date'], appleDb['Open'])\n",
    "# axs[0].set_title('Apple Db')\n",
    "# axs[1].plot(americanAirlineDb['Date'], americanAirlineDb['Open'])\n",
    "# axs[1].set_title('American Airline Db')\n",
    "# axs[2].plot(apacheCorpDb['Date'], apacheCorpDb['Open'])\n",
    "# axs[2].set_title('Apache Corp Db')\n",
    "\n",
    "# # adjust the space between subplots\n",
    "# plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# # add a common x-label and y-label to the collage\n",
    "# fig.text(0.5, 0.04, 'Date', ha='center')\n",
    "# fig.text(0.04, 0.5, 'Open', va='center', rotation='vertical')\n",
    "\n",
    "# # display the plot\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Graphs - The highest price the stock reached during the day. This helps to identify how volatile the stock is and how much potential upside there is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plotting the closing price of the stocks\n",
    "\n",
    "# # create a figure and axis objects with 3 rows and 1 column\n",
    "# fig, axs = plt.subplots(3, 1, figsize=(8, 10))\n",
    "\n",
    "# # plot the high prices for each company on each axis object\n",
    "# axs[0].plot(appleDb['Date'], appleDb['High'])\n",
    "# axs[0].set_title('Apple Db')\n",
    "# axs[1].plot(americanAirlineDb['Date'], americanAirlineDb['High'])\n",
    "# axs[1].set_title('American Airline Db')\n",
    "# axs[2].plot(apacheCorpDb['Date'], apacheCorpDb['High'])\n",
    "# axs[2].set_title('Apache Corp Db')\n",
    "\n",
    "# # adjust the space between subplots\n",
    "# plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# # add a common x-label and y-label to the collage\n",
    "# fig.text(0.5, 0.04, 'Date', ha='center')\n",
    "# fig.text(0.04, 0.5, 'High', va='center', rotation='vertical')\n",
    "\n",
    "# # display the plot\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low Graphs -  The lowest price the stock reached during the day. This helps you evaluate how risky the stock is and how much potential downside there is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plotting the closing price of the stocks\n",
    "\n",
    "# # create a figure and axis objects with 3 rows and 1 column\n",
    "# fig, axs = plt.subplots(3, 1, figsize=(8, 10))\n",
    "\n",
    "# # plot the low prices for each company on each axis object\n",
    "# axs[0].plot(appleDb['Date'], appleDb['Low'])\n",
    "# axs[0].set_title('Apple Db')\n",
    "# axs[1].plot(americanAirlineDb['Date'], americanAirlineDb['Low'])\n",
    "# axs[1].set_title('American Airline Db')\n",
    "# axs[2].plot(apacheCorpDb['Date'], apacheCorpDb['Low'])\n",
    "# axs[2].set_title('Apache Corp Db')\n",
    "\n",
    "# # adjust the space between subplots\n",
    "# plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# # add a common x-label and y-label to the collage\n",
    "# fig.text(0.5, 0.04, 'Date', ha='center')\n",
    "# fig.text(0.04, 0.5, 'Low', va='center', rotation='vertical')\n",
    "\n",
    "# # display the plot\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closing Graphs - The closing price of the stock on that day. This can helps to evaluate how the stock performed over the course of the day and whether it ended up in positive or negative territory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plotting the closing price of the stocks\n",
    "\n",
    "# # create a figure and axis objects with 3 rows and 1 column\n",
    "# fig, axs = plt.subplots(3, 1, figsize=(8, 10))\n",
    "\n",
    "# # plot the close prices for each company on each axis object\n",
    "# axs[0].plot(appleDb['Date'], appleDb['Close'])\n",
    "# axs[0].set_title('Apple Db')\n",
    "# axs[1].plot(americanAirlineDb['Date'], americanAirlineDb['Close'])\n",
    "# axs[1].set_title('American Airline Db')\n",
    "# axs[2].plot(apacheCorpDb['Date'], apacheCorpDb['Close'])\n",
    "# axs[2].set_title('Apache Corp Db')\n",
    "\n",
    "# # adjust the space between subplots\n",
    "# plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# # add a common x-label and y-label to the collage\n",
    "# fig.text(0.5, 0.04, 'Date', ha='center')\n",
    "# fig.text(0.04, 0.5, 'Closing', va='center', rotation='vertical')\n",
    "\n",
    "# # display the plot\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volume Graphs - The total number of shares of the stock that were traded on that day. This helps to evaluate how active the market is for the stock and how much interest there is from other investors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plotting the closing price of the stocks\n",
    "\n",
    "# # create a figure and axis objects with 3 rows and 1 column\n",
    "# fig, axs = plt.subplots(3, 1, figsize=(8, 10))\n",
    "\n",
    "# # plot the close prices for each company on each axis object\n",
    "# axs[0].plot(appleDb['Date'], appleDb['Volume'])\n",
    "# axs[0].set_title('Apple Db')\n",
    "# axs[1].plot(americanAirlineDb['Date'], americanAirlineDb['Volume'])\n",
    "# axs[1].set_title('American Airline Db')\n",
    "# axs[2].plot(apacheCorpDb['Date'], apacheCorpDb['Volume'])\n",
    "# axs[2].set_title('Apache Corp Db')\n",
    "\n",
    "# # adjust the space between subplots\n",
    "# plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# # add a common x-label and y-label to the collage\n",
    "# fig.text(0.5, 0.04, 'Date', ha='center')\n",
    "# fig.text(0.04, 0.5, 'Volume', va='center', rotation='vertical')\n",
    "\n",
    "# # display the plot\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding extra features into the dataframe to help the AI stock advisor to make better predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def priceChange(df):\n",
    "    df['Price Change'] = df['Close'] - df['Close'].shift(1) #taking the closing price of the current day and subtracting it from the closing price of the previous day\n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding the Price Change in the data frame it will illustrate the percentage change between the previous closing price (of the previous day) and closing price of that day for each day. This can provide insight into the daily market sentiment of the stock. The reason for this is that the difference between the previous closing price and the closing price for a given day provides a good indication of the direction and magnitude of price movements for that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def priceVolatility(df):\n",
    "    df['Price Volatility'] = df['High'] - df['Low'] #taking the high price of the day and subtracting it from the low price of the day\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding the Price Volatility to the dataframe it refers to how much the price of a stock moves up and down over a certain period. The more a stock's price fluctuates (changes frequently), the higher its volatility. \n",
    "\n",
    "Note that the Volatility means the amount of uncertainty or risk related to the size of changes in a security's value\n",
    "\n",
    "Volatility can be caused by various factors, such as changes in market conditions, news events, or investor sentiment. High volatility can indicate that the stock is more risky or uncertain, while low volatility can suggest that the stock is more stable and predictable. \n",
    "\n",
    "To work out the volatility of a stock taking the difference between the high and low prices of a stock. As stated before, the higher the difference, the more volatility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the SMA for the closing price of the stocks\n",
    "\n",
    "def SMA(df,windowValue):\n",
    "    df['SMA'] = tb.SMA(df['Close'], windowValue)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TEMA(df,windowValue):\n",
    "    df['EMA'] = tb.EMA(df['Close'], windowValue)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot SMA vs Closing Price\n",
    "# fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,8))\n",
    "# ax1.plot(appleDb['Date'],appleDb['Close'], label='Closing Price')\n",
    "# ax1.plot(appleDb['Date'],appleDb['SMA'], label='SMA')\n",
    "# ax1.set_title('SMA vs Closing Price')\n",
    "# ax1.legend()\n",
    "\n",
    "# # Plot TEMA vs Closing Price\n",
    "# ax2.plot(appleDb['Date'],appleDb['Close'], label='Closing Price')\n",
    "# ax2.plot(appleDb['Date'],appleDb['TEMA'], label='TEMA')\n",
    "# ax2.set_title('TEMA vs Closing Price')\n",
    "# ax2.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving Averages: \n",
    "\n",
    "\n",
    "Moving averages are a trend-following indicator that smooths out price fluctuations over a given period of time. They can be used to identify trends and potential reversal points in the stock's price."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Simple Moving Average (SMA) is useful as it is a way to calculate the average price of a stock over a specific period of time. For example, if you want to calculate the SMA over the last 10 days, you would add up the prices of the stock from each of the last 10 days and divide that sum by 10.\n",
    "\n",
    "The SMA is called \"moving\" because as each new day's price is added to the calculation, the oldest price is dropped, and the SMA \"moves\" to reflect the new set of prices.\n",
    "\n",
    "The SMA can help to smooth out the daily fluctuations in the stock price and give you a better idea of the overall trend. If the SMA is moving up, it means that the stock price is generally increasing, and if the SMA is moving down, it means that the stock price is generally decreasing.\n",
    "\n",
    "the simple moving average (SMA) is a calculation that helps to identify trends in the price of a stock over a specific time period. It is calculated by adding up the prices over a certain number of days, and then dividing by the number of days.\n",
    "\n",
    "For example, if you want to calculate the 20-day SMA, you would add up the closing prices for the past 20 days and then divide by 20.\n",
    "\n",
    " The closing price is used because it is the final price of a trading day and is considered to be the most important price point for that day. Using closing prices over a period of time helps to reduce the impact of short-term price fluctuations and provides a more accurate picture of the trend over that time period.\n",
    "\n",
    "The TEMA moving average is a technical analysis indicator that uses a triple exponential moving average to reduce the lag of the standard exponential moving average (EMA).\n",
    "\n",
    "To calculate the TEMA, you first calculate a single EMA for a given period, then you calculate a second EMA of that first EMA, and then a third EMA of that second EMA. The formula for the TEMA is:\n",
    "\n",
    "TEMA = 3 * (EMA1 - EMA2) + EMA3\n",
    "\n",
    "where EMA1 is the first EMA, EMA2 is the second EMA of the first EMA, and EMA3 is the third EMA of the second EMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADX(df):\n",
    "    df['ADX5'] = tb.ADX(df['High'],df['Low'], df['Close'], timeperiod=5)\n",
    "    df['ADX10'] = tb.ADX(df['High'],df['Low'], df['Close'], timeperiod=10)\n",
    "    df['ADX20'] = tb.ADX(df['High'],df['Low'], df['Close'], timeperiod=20)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Plot Various ADX values\n",
    "# fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(10,8))\n",
    "# ax1.plot(appleDb['Date'],appleDb['High'], label='High Price')\n",
    "# ax1.plot(appleDb['Date'],appleDb['Low'], label='Low Price')\n",
    "# ax1.plot(appleDb['Date'],appleDb['Close'], label='Closing Price')\n",
    "# ax1.plot(appleDb['Date'],appleDb['ADX20'], label='ADX20')\n",
    "# ax1.set_title('ADX20')\n",
    "# ax1.legend()\n",
    "\n",
    "# ax2.plot(appleDb['Date'],appleDb['High'], label='High Price')\n",
    "# ax2.plot(appleDb['Date'],appleDb['Low'], label='Low Price')\n",
    "# ax2.plot(appleDb['Date'],appleDb['Close'], label='Closing Price')\n",
    "# ax2.plot(appleDb['Date'],appleDb['ADX10'], label='ADX10')\n",
    "# ax2.set_title('ADX10')\n",
    "# ax2.legend()\n",
    "\n",
    "# ax3.plot(appleDb['Date'],appleDb['High'], label='High Price')\n",
    "# ax3.plot(appleDb['Date'],appleDb['Low'], label='Low Price')\n",
    "# ax3.plot(appleDb['Date'],appleDb['Close'], label='Closing Price')\n",
    "# ax3.plot(appleDb['Date'],appleDb['ADX5'], label='ADX5')\n",
    "# ax3.set_title('ADX5')\n",
    "# ax3.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Directional Movement Index(Momentum Indicator)\n",
    "\n",
    "ADX can be used to help measure the overall strength of a trend. The ADX indicator is an average of expanding price range values. The Average Directional Movement Index (ADX) is a technical indicator that is based on the high, low, and close prices of a stock. The ADX helps to measure the strength of a trend, whether it's up or down, and its overall momentum.\n",
    "\n",
    "The ADX is calculated using a formula that takes into account the price movements over a specific period of time. The indicator is based on the difference between the high and low prices and the current closing price. The ADX is usually plotted on a chart as a line that ranges between 0 and 100, with a higher value indicating a stronger trend.\n",
    "\n",
    "To calculate the ADX for a stock, you would first need to determine the time period for which you want to calculate the indicator. This can vary depending on your strategy and the volatility of the stock you are analyzing. Once you have determined the time period, you can then use the high, low, and close prices for that period to calculate the ADX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ATR(df):\n",
    "    df['ATR5'] = tb.ATR(df['High'], df['Low'], df['Close'], timeperiod=5)\n",
    "    df['ATR10'] = tb.ATR(df['High'], df['Low'], df['Close'], timeperiod=10)\n",
    "    return df\n",
    "\n",
    "# ax1.plot(appleDb['Date'],appleDb['Close'], label='Closing Price')\n",
    "# ax1.plot(appleDb['Date'],appleDb['High'], label='High Price')\n",
    "# ax1.plot(appleDb['Date'],appleDb['Low'], label='Low Price')\n",
    "# ax1.plot(appleDb['Date'],appleDb['ATR'], label='ATR',color='red')\n",
    "# ax1.set_title('ATR vs Closing Price vs High Price vs Low Price')\n",
    "# ax1.legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average True Range (ATR): ATR is a measure of volatility that takes into account the daily price range of the stock. It can help predict potential price movement and can be useful for setting stop-loss orders.\n",
    "\n",
    "The Average True Range (ATR) is another technical indicator that is commonly used by traders and investors to measure the volatility of a stock. While the ADX measures the strength of a trend, the ATR measures the volatility of the stock price.\n",
    "\n",
    "The ATR is calculated using the difference between the high and low prices of a stock over a specific period of time. It takes into account any gaps or limit moves that may have occurred during that time period. The ATR is usually expressed in points or as a percentage of the stock price.\n",
    "\n",
    "One key difference between the ADX and ATR is that the ADX is used to measure the strength of a trend, while the ATR is used to measure the volatility of the stock price. Another difference is that the ADX is calculated using the high, low, and close prices, while the ATR is calculated using only the high and low prices.\n",
    "\n",
    "Both the ADX and ATR are useful indicators that can help traders and investors make informed decisions about buying and selling stocks. The ADX can help to identify strong trends, while the ATR can help to identify potential changes in volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSI(df):\n",
    "    df['RSI'] = tb.RSI(df['Close'], timeperiod=10)\n",
    "    return df\n",
    "\n",
    "\n",
    "# appleDb[['RSI']].plot(figsize=(12,10),marker='o')\n",
    "# plt.axhline(y=30, color='green', linestyle='-')\n",
    "# plt.axhline(y=70, color='red', linestyle='-')\n",
    "# x = np.arange(0, len(appleDb['RSI']),0.1)\n",
    "# appleDb['RSI'].tail(10)\n",
    "# plt.title(\"Apple RSI\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative Strength Index (RSI): an indicator used in finance to help investors and traders understand whether a stock is overbought or oversold. It is a measure of the stock's recent price changes, and is expressed as a number between 0 and 100.\n",
    "\n",
    "The RSI works by comparing the average price gains of a stock to its average price losses over a specific time period (usually 14 days). If the average gains are higher than the average losses, the RSI will be higher, indicating that the stock is in an uptrend. If the average losses are higher than the average gains, the RSI will be lower, indicating that the stock is in a downtrend.\n",
    "\n",
    "Traders use the RSI to identify potential buy and sell signals. When the RSI is above 70, it is considered overbought, which means that the stock may be due for a price correction. When the RSI is below 30, it is considered oversold, which means that the stock may be undervalued and due for a price increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appleDb['Upper_BBand'], appleDb['Mid_BBand'], appleDb['Lower_BBand'] = tb.BBANDS(appleDb['Close'], timeperiod =20)\n",
    "# appleDb[['Close','Mid_BBand','Upper_BBand','Lower_BBand']].plot(figsize= (12,10))\n",
    "# plt.title(\"Apple Bollinger Bands\")\n",
    "# plt.show()\n",
    "\n",
    "def bbands(df):\n",
    "    df['Upper_BBand5'], df['Mid_BBand5'], df['Lower_BBand5'] = tb.BBANDS(df['Close'], timeperiod =5)\n",
    "    df['Upper_BBand10'], df['Mid_BBand10'], df['Lower_BBand10'] = tb.BBANDS(df['Close'], timeperiod =10)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bollinger Bands is a tool that helps traders understand how much a stock's price tends to change over time. It consists of three lines on a chart: a middle line that represents the average price of the stock over a certain period (usually 20 days), and two additional lines that are drawn above and below the middle line.\n",
    "\n",
    "These upper and lower lines are drawn at a distance that represents how much the stock's price has varied in the past. The distance is based on the stock's standard deviation, which is a measure of how much its price has fluctuated in the past.\n",
    "\n",
    "When the stock's price is moving within the Bollinger Bands, it is considered to be trading within a normal range. However, when the stock's price moves above the upper line, it is considered to be overbought, meaning it may be due for a price correction. Conversely, when the stock's price moves below the lower line, it is considered to be oversold, meaning it may be due for a price increase.\n",
    "\n",
    "Bollinger Bands can be a useful tool for traders to help identify potential buy and sell signals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the MACD and Signal Line indicators\n",
    "# appleDb[\"MACD\"], appleDb[\"macd_signal\"], appleDb[\"macd_hist\"] = tb.MACD(appleDb[\"Close\"], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "# appleDb[['MACD','macd_signal','macd_hist']].plot(figsize= (12,10))\n",
    "# plt.title(\"Apple MACD\")\n",
    "# plt.show()\n",
    "\n",
    "def MACD(df):\n",
    "    df[\"MACD\"], df[\"macd_signal\"], df[\"macd_hist\"] = tb.MACD(df[\"Close\"], fastperiod=5, slowperiod=20, signalperiod=9)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving Average Convergence Divergence (MACD): The MACD is a trend-following indicator that calculates the difference between two moving averages of the stock's price. It can help identify changes in the stock's trend and can be used to generate buy or sell signals.\n",
    "\n",
    "The MACD is a trend-following indicator that measures the difference between a short-term moving average and a long-term moving average. It is typically used with a 12-day and 26-day moving average, and a 9-day signal line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the On Balance Volume indicator\n",
    "# appleDb['OBV'] = tb.OBV(appleDb['Close'], appleDb['Volume'])\n",
    "# appleDb[['OBV','Close']].plot(figsize= (12,10))\n",
    "# plt.title(\"Apple OBV\")\n",
    "# plt.show()\n",
    "# print(appleDb.to_string())\n",
    "\n",
    "def OBV(df):\n",
    "    df['OBV'] = tb.OBV(df['Close'], df['Volume'])\n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Balance Volume (OBV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the Chaikin Money Flow indicator\n",
    "# appleDb['MFI'] = tb.MFI(appleDb['High'], appleDb['Low'], appleDb['Close'], appleDb['Volume'])\n",
    "# appleDb[['MFI','Close']].plot(figsize= (12,10))\n",
    "# plt.title(\"Apple MFI\")\n",
    "# plt.show()\n",
    "# print(appleDb.to_string())\n",
    "\n",
    "def MFI(df):\n",
    "    df['MFI'] = tb.MFI(df['High'], df['Low'], df['Close'], df['Volume'])\n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Money Flow Index (MFI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction of Close Price after a certain amount of days\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closePriceAfter(df):\n",
    "    df['ClosePriceAfter10Days'] = df['Close'].shift(-10)\n",
    "    df['ClosePriceAfter30Days'] = df['Close'].shift(-30)\n",
    "    df['ClosePriceAfter60Days'] =df['Close'].shift(-60)\n",
    "    df['ClosePriceAfter120Days'] = df['Close'].shift(-120)\n",
    "    df['ClosePriceAfter365Days'] =df['Close'].shift(-365)\n",
    "    return df\n",
    "\n",
    "def differencePriceAfter(df):\n",
    "    df['DifferenceAfter10Days'] = df['ClosePriceAfter10Days']-  df['Close']\n",
    "    df['DifferenceAfter30Days'] = df['ClosePriceAfter30Days']-  df['Close']\n",
    "    df['DifferenceAfter60Days'] = df['ClosePriceAfter60Days'] -  df['Close']\n",
    "    df['DifferenceAfter120Days'] = df['ClosePriceAfter120Days']- df['Close']\n",
    "    df['DifferenceAfter365Days'] = df['ClosePriceAfter365Days'] -  df['Close']\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def obtainingExtraFeatures(df,windowValue):\n",
    "    df = priceChange(df) \n",
    "    df = priceVolatility(df)\n",
    "    df = SMA(df,windowValue)\n",
    "    df = TEMA(df,windowValue)\n",
    "    df = ADX(df)\n",
    "    df = ATR(df)\n",
    "    df = RSI(df)\n",
    "    df = bbands(df)\n",
    "    df = MACD(df)\n",
    "    df = OBV(df)\n",
    "    df = MFI(df)\n",
    "    df = closePriceAfter(df)\n",
    "    df = differencePriceAfter(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "appleDb = obtainingExtraFeatures(appleDb,10)\n",
    "\n",
    "print(appleDb.to_string())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the dataset into training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splittingTheDataset(database,nameOfPredictedColumn,predictionDays,testSize,randomState):\n",
    "    database = database.iloc[:-predictionDays]\n",
    "    print(database)\n",
    "    inputFeatureDatabase = database[['Open','High','Low','Close','Volume','Dividends','Stock Splits','Price Change','Price Volatility','SMA','EMA','ADX5','ADX10','ADX20',\n",
    "                                     'ATR5','ATR10','RSI', 'Upper_BBand5','Mid_BBand5','Lower_BBand5','Upper_BBand10','Mid_BBand10','Lower_BBand10','MACD','macd_signal','macd_hist',\n",
    "                                     'OBV','MFI']]\n",
    "\n",
    "    targetOutputVariable = database[[nameOfPredictedColumn]]\n",
    "    \n",
    "    #splitting the dataset into train and test using the train_test_split function -This splits the data into 80% training data and 20% testing data\n",
    "    inputFeatureDatabase_Train, inputFeatureDatabase_Test, targetOutputVariable_Train, targetOutputVariable_Test = train_test_split(inputFeatureDatabase, targetOutputVariable, test_size=testSize, random_state=randomState)\n",
    "    \n",
    "    return inputFeatureDatabase_Train, inputFeatureDatabase_Test, targetOutputVariable_Train, targetOutputVariable_Test \n",
    "\n",
    "def obtainRecentRecords(database):\n",
    "    # Convert Date column to datetime object\n",
    "    database['Date'] = pd.to_datetime(database['Date'], utc= True) #set the utc parameter to True to ensure that the timezone information is preserved\n",
    "\n",
    "    # Filter rows based on condition - starting from the year 2010\n",
    "    database = database[pd.to_datetime(database['Date']).dt.year >= 2010]\n",
    "    \n",
    "\n",
    "    return database\n",
    "\n",
    "appleDb = obtainRecentRecords(appleDb)\n",
    "print(appleDb.to_string())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_test_split function splits the input data into two subsets: one for training the model and the other for testing the model. The function returns four outputs, which are:\n",
    "\n",
    "inputFeatureDatabase_Train: the feature data for training the model.\n",
    "inputFeatureDatabase_Test: the feature data for testing the model.\n",
    "targetOutputVariable_Train: the target data (labels) for training the model.\n",
    "targetOutputVariable_Test: the target data (labels) for testing the model.\n",
    "So, inputFeatureDatabase_Train and targetOutputVariable_Train are used to train the model, while inputFeatureDatabase_Test and targetOutputVariable_Test are used to evaluate the performance of the model on unseen data.\n",
    "\n",
    "The random_state=42 parameter in the train_test_split() function sets the random seed for the data splitting process. This ensures that the data is split in the same way every time the code is run, which is useful for reproducibility and debugging purposes. The value 42 is an arbitrary number and can be any non-negative integer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the parameters for the AI Model "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters provided above are used to configure a machine learning model that uses the LightGBM library for gradient boosting. Here's an explanation of each parameter:\n",
    "\n",
    "task: The task parameter specifies whether the model should be used for training ('train'), prediction ('predict'), or other tasks.\n",
    "\n",
    "boosting: The boosting parameter specifies the type of boosting algorithm to use. In this case, 'gbdt' stands for Gradient Boosting Decision Tree, which is a popular algorithm for supervised learning tasks like regression and classification.\n",
    "\n",
    "objective: The objective parameter specifies the loss function to be optimized during training. In this case, 'regression' indicates that the model is being trained to perform a regression task, where the goal is to predict a continuous numerical value.\n",
    "\n",
    "num_leaves: The num_leaves parameter controls the number of leaves (or nodes) in each decision tree of the gradient boosting algorithm. Increasing this value can lead to more complex models with better performance, but also increases the risk of overfitting.\n",
    "\n",
    "learning_rate: The learning_rate parameter controls the step size at each iteration of the gradient boosting algorithm. A smaller learning rate can lead to more accurate models, but also requires more iterations to converge.\n",
    "\n",
    "metric: The metric parameter specifies the evaluation metric(s) to be used during training. In this case, {'l2', 'l1'} indicates that the mean squared error (l2) and mean absolute error (l1) will be used to evaluate the model's performance during training.\n",
    "\n",
    "verbose: The verbose parameter controls the amount of output printed during training. A value of -1 means that no output will be printed.\n",
    "\n",
    "Overall, these parameters are used to configure a gradient boosting model that is optimized for a regression task, using a decision tree as the base estimator. The specific values for each parameter may need to be tuned for optimal performance on a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laoding data into lightgbm -  taking the training and testing data and converting them into formats that can be used by the LightGBM model for training and evaluation.\n",
    "def changingIntoLightGBMformat(inputFeatureDatasetTrain, targetOutputVariable_Train, inputFeatureDatasetTest, targetOutputVariable_Test):\n",
    "    lgb_ToTrainModel = lgb.Dataset(inputFeatureDatasetTrain, label = targetOutputVariable_Train)\n",
    "    lgb_evalualtePerformanceDuringTraining = lgb.Dataset(inputFeatureDatasetTest, label = targetOutputVariable_Test, reference=lgb_ToTrainModel)\n",
    "    return lgb_ToTrainModel, lgb_evalualtePerformanceDuringTraining\n",
    "\n",
    "# fitting the model\n",
    "\n",
    "def buildModel(params, lgb_ToTrainModel, lgb_evalualtePerformanceDuringTraining, early_stopping_rounds):\n",
    "    model = lgb.train(params,\n",
    "                 train_set=lgb_ToTrainModel,\n",
    "                 valid_sets=lgb_evalualtePerformanceDuringTraining,\n",
    "                 early_stopping_rounds=early_stopping_rounds)\n",
    "    return model\n",
    "\n",
    "#predicting the output values\n",
    "def prediction(model, inputFeatureDatabase_Test):\n",
    "    predictedOutputValues = model.predict(inputFeatureDatabase_Test)\n",
    "    return predictedOutputValues\n",
    "\n",
    "\n",
    "#accuracy check - using the mean squared error and root mean squared error\n",
    "def calculateMSERmse(targetOutputVariable_Test, predictedOutputValues):\n",
    "    mse = mean_squared_error(targetOutputVariable_Test, predictedOutputValues)\n",
    "    rmse = mse**(0.5)\n",
    "\n",
    "    return mse, rmse\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lgb.Dataset(inputFeatureDatabase_Train, targetOutputVariable_Train) is creating a LightGBM dataset object called lgb_ToTrainModel using the training data, inputFeatureDatabase_Train and targetOutputVariable_Train. This dataset will be used to train the LightGBM model.\n",
    "lgb.Dataset(inputFeatureDatabase_Test, targetOutputVariable_Test, reference=lgb_ToTrainModel) is creating another LightGBM dataset object called lgb_evalualtePerformanceDuringTraining using the testing data, inputFeatureDatabase_Test and targetOutputVariable_Test. This dataset will be used to evaluate the performance of the LightGBM model during training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why minimising the loss was chosen rather than maximising the prediction:\n",
    "\n",
    "Maximizing the prediction means trying to make the model's predictions as accurate as possible, regardless of the cost of making a mistake. In other words, the focus is on optimizing the model's ability to make correct predictions, without considering the cost of getting things wrong. This approach is often used when the consequences of making a mistake are not severe, or when the cost of getting things right is much higher than the cost of getting things wrong.\n",
    "\n",
    "On the other hand, minimizing the loss means trying to make the model's predictions as accurate as possible, while also minimizing the cost of making a mistake. In this approach, the focus is on optimizing the model's ability to balance the trade-off between making correct predictions and avoiding mistakes that have high costs. This approach is often used when the cost of making a mistake is high, such as in medical diagnosis or financial forecasting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Model for 30 Day Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model for 30 Day Prediction\n",
    "def objective(trial, lgb_ToTrainModel1_30Days, lgb_evalualtePerformanceDuringTrainingModel1_30Days, inputFeatureDatabase_TestModel1_30Days, targetOutputVariable_TestModel1_30Days):\n",
    "    params = {\n",
    "        'task': 'train', \n",
    "        'boosting': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 5, 50),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'metric': {'l2', 'l1'},\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    # Train the model and make predictions\n",
    "    model1_30Days = buildModel(params, lgb_ToTrainModel1_30Days, lgb_evalualtePerformanceDuringTrainingModel1_30Days, 30)\n",
    "\n",
    "    predictedOutputValuesModel1_30Days = prediction(model1_30Days, inputFeatureDatabase_TestModel1_30Days)\n",
    "\n",
    "    # Calculate the error\n",
    "    mse_combined, rmse = calculateMSERmse(targetOutputVariable_TestModel1_30Days, predictedOutputValuesModel1_30Days)\n",
    "\n",
    "    # Return the evaluation metric (MSE) as the objective value to minimize\n",
    "    return mse_combined\n",
    "\n",
    "# Prepare the training and testing data for model 1 - 30 days\n",
    "inputFeatureDatabase_TrainModel1_30Days, inputFeatureDatabase_TestModel1_30Days, targetOutputVariable_TrainModel1_30Days, targetOutputVariable_TestModel1_30Days  = splittingTheDataset(appleDb, 'DifferenceAfter30Days', 30, 0.2, 42)\n",
    "\n",
    "lgb_ToTrainModel1_30Days, lgb_evalualtePerformanceDuringTrainingModel1_30Days = changingIntoLightGBMformat(inputFeatureDatabase_TrainModel1_30Days, targetOutputVariable_TrainModel1_30Days, inputFeatureDatabase_TestModel1_30Days, targetOutputVariable_TestModel1_30Days)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, lgb_ToTrainModel1_30Days, lgb_evalualtePerformanceDuringTrainingModel1_30Days, inputFeatureDatabase_TestModel1_30Days, targetOutputVariable_TestModel1_30Days), n_trials=100, show_progress_bar=False)\n",
    "\n",
    "best_paramsForModel1 = study.best_params\n",
    "print(\"Best Parameters:\", best_paramsForModel1)\n",
    "\n",
    "# Re-run the model with the best parameters\n",
    "#Number of training epochs: In machine learning, an epoch is one complete pass through the entire training dataset. If 30 represents the number of epochs, then each model will be trained for 30 iterations over the training data. The number of epochs can affect the accuracy of the model. Too few epochs can result in underfitting of the model, whereas too many epochs can lead to overfitting.\n",
    "best_modelForModel1 = buildModel(best_paramsForModel1, lgb_ToTrainModel1_30Days, lgb_evalualtePerformanceDuringTrainingModel1_30Days, 30) \n",
    "predicted_outputForModel1 = prediction(best_modelForModel1, inputFeatureDatabase_TestModel1_30Days)\n",
    "\n",
    "# Calculate the error with the best model\n",
    "best_mseForModel1, best_rmseForModel1 = calculateMSERmse(targetOutputVariable_TestModel1_30Days, predicted_outputForModel1)\n",
    "\n",
    "print(\"Best MSE:\", best_mseForModel1)\n",
    "print(\"Best RMSE:\", best_rmseForModel1)\n",
    "\n",
    "\n",
    "#save the model \n",
    "filename = \"model1.txt\"\n",
    "pickle.dump(best_modelForModel1, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, lgb_ToTrainModel2_60Days, lgb_evalualtePerformanceDuringTrainingModel2_60Days, inputFeatureDatabase_TestModel2_60Days, targetOutputVariable_TestModel2_60Days):\n",
    "    params = {\n",
    "        'task': 'train', \n",
    "        'boosting': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 5, 50),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'metric': {'l2', 'l1'},\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    # Train the model and make predictions\n",
    "    model2_60Days = buildModel(params, lgb_ToTrainModel2_60Days, lgb_evalualtePerformanceDuringTrainingModel2_60Days, 30)\n",
    "    predictedOutputValuesModel2_60Days = prediction(model2_60Days, inputFeatureDatabase_TestModel2_60Days)\n",
    "\n",
    "    # Calculate the error\n",
    "    mse_combined, rmse = calculateMSERmse(targetOutputVariable_TestModel2_60Days, predictedOutputValuesModel2_60Days)\n",
    "\n",
    "    # Return the evaluation metric (MSE) as the objective value to minimize\n",
    "    return mse_combined\n",
    "\n",
    "# Prepare the training and testing data for model 1 - 30 days\n",
    "inputFeatureDatabase_TrainModel2_60Days, inputFeatureDatabase_TestModel2_60Days, targetOutputVariable_TrainModel2_60Days, targetOutputVariable_TestModel2_60Days  = splittingTheDataset(appleDb, 'DifferenceAfter60Days', 60, 0.2, 42)\n",
    "\n",
    "lgb_ToTrainModel2_60Days, lgb_evalualtePerformanceDuringTrainingModel2_60Days = changingIntoLightGBMformat(inputFeatureDatabase_TrainModel2_60Days, targetOutputVariable_TrainModel2_60Days, inputFeatureDatabase_TestModel2_60Days, targetOutputVariable_TestModel2_60Days)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, lgb_ToTrainModel2_60Days, lgb_evalualtePerformanceDuringTrainingModel2_60Days, inputFeatureDatabase_TestModel2_60Days, targetOutputVariable_TestModel2_60Days), n_trials=100)\n",
    "\n",
    "best_paramsForModel2 = study.best_params\n",
    "print(\"Best Parameters:\", best_paramsForModel2)\n",
    "\n",
    "# Re-run the model with the best parameters\n",
    "best_modelForModel2 = buildModel(best_paramsForModel2, lgb_ToTrainModel2_60Days, lgb_evalualtePerformanceDuringTrainingModel2_60Days, 30)\n",
    "predicted_outputForModel2 = prediction(best_modelForModel2, inputFeatureDatabase_TestModel2_60Days)\n",
    "\n",
    "# Calculate the error with the best model\n",
    "best_mseForModel2, best_rmseForModel2 = calculateMSERmse(targetOutputVariable_TestModel2_60Days, predicted_outputForModel2)\n",
    "\n",
    "# Print intermediate results\n",
    "print(\"Best MSE:\", best_mseForModel2)\n",
    "print(\"Best RMSE:\", best_rmseForModel2)\n",
    "\n",
    "#save the model \n",
    "filename = \"model2.txt\"\n",
    "pickle.dump(best_modelForModel2, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for 120 days prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, lgb_ToTrainModel3_120Days, lgb_evalualtePerformanceDuringTrainingModel3_120Days, inputFeatureDatabase_TestModel3_120Days, targetOutputVariable_TestModel3_120Days):\n",
    "    params = {\n",
    "        'task': 'train', \n",
    "        'boosting': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 5, 50),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'metric': {'l2', 'l1'},\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    # Train the model and make predictions\n",
    "    model3_120Days = buildModel(params, lgb_ToTrainModel3_120Days, lgb_evalualtePerformanceDuringTrainingModel3_120Days, 30)\n",
    "    predictedOutputValuesModel3_120Days = prediction(model3_120Days, inputFeatureDatabase_TestModel3_120Days)\n",
    "\n",
    "    # Calculate the error\n",
    "    mse_combined, rmse = calculateMSERmse(targetOutputVariable_TestModel3_120Days, predictedOutputValuesModel3_120Days)\n",
    "\n",
    "    # Return the evaluation metric (MSE) as the objective value to minimize\n",
    "    return mse_combined\n",
    "\n",
    "# Prepare the training and testing data for model 3 - 120 days\n",
    "inputFeatureDatabase_TrainModel3_120Days, inputFeatureDatabase_TestModel3_120Days, targetOutputVariable_TrainModel3_120Days, targetOutputVariable_TestModel3_120Days  = splittingTheDataset(appleDb, 'DifferenceAfter120Days', 120, 0.2, 42)\n",
    "\n",
    "lgb_ToTrainModel3_120Days, lgb_evalualtePerformanceDuringTrainingModel3_120Days = changingIntoLightGBMformat(inputFeatureDatabase_TrainModel3_120Days, targetOutputVariable_TrainModel3_120Days, inputFeatureDatabase_TestModel3_120Days, targetOutputVariable_TestModel3_120Days)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, lgb_ToTrainModel3_120Days, lgb_evalualtePerformanceDuringTrainingModel3_120Days, inputFeatureDatabase_TestModel3_120Days, targetOutputVariable_TestModel3_120Days), n_trials=100)\n",
    "\n",
    "best_paramsForModel3 = study.best_params\n",
    "print(\"Best Parameters:\", best_paramsForModel3)\n",
    "\n",
    "# Re-run the model with the best parameters\n",
    "best_modelForModel3 = buildModel(best_paramsForModel3, lgb_ToTrainModel3_120Days, lgb_evalualtePerformanceDuringTrainingModel3_120Days, 30)\n",
    "predicted_outputForModel3 = prediction(best_modelForModel3, inputFeatureDatabase_TestModel3_120Days)\n",
    "\n",
    "# Calculate the error with the best model\n",
    "best_mseForModel3, best_rmseForModel3 = calculateMSERmse(targetOutputVariable_TestModel3_120Days, predicted_outputForModel3)\n",
    "\n",
    "# Print intermediate results\n",
    "print(\"Best MSE:\", best_mseForModel3)\n",
    "print(\"Best RMSE:\", best_rmseForModel3)\n",
    "\n",
    "#save the model \n",
    "filename = \"model3.txt\"\n",
    "pickle.dump(best_modelForModel3, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the model for Prediction 365 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, lgb_ToTrainModel4_365Days, lgb_evalualtePerformanceDuringTrainingModel4_365Days, inputFeatureDatabase_TestModel4_365Days, targetOutputVariable_TestModel4_365Days):\n",
    "    params = {\n",
    "        'task': 'train', \n",
    "        'boosting': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 5, 50),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'metric': {'l2', 'l1'},\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    # Train the model and make predictions\n",
    "    model4_365Days = buildModel(params, lgb_ToTrainModel4_365Days, lgb_evalualtePerformanceDuringTrainingModel4_365Days, 30)\n",
    "    predictedOutputValuesModel4_365Days = prediction(model4_365Days, inputFeatureDatabase_TestModel4_365Days)\n",
    "\n",
    "    # Calculate the error\n",
    "    mse_combined, rmse = calculateMSERmse(targetOutputVariable_TestModel4_365Days, predictedOutputValuesModel4_365Days)\n",
    "\n",
    "    # Return the evaluation metric (MSE) as the objective value to minimize\n",
    "    return mse_combined\n",
    "\n",
    "# Prepare the training and testing data for model 4 - 365 days\n",
    "inputFeatureDatabase_TrainModel4_365Days, inputFeatureDatabase_TestModel4_365Days, targetOutputVariable_TrainModel4_365Days, targetOutputVariable_TestModel4_365Days  = splittingTheDataset(appleDb, 'DifferenceAfter365Days', 365, 0.2, 42)\n",
    "\n",
    "lgb_ToTrainModel4_365Days, lgb_evalualtePerformanceDuringTrainingModel4_365Days = changingIntoLightGBMformat(inputFeatureDatabase_TrainModel4_365Days, targetOutputVariable_TrainModel4_365Days, inputFeatureDatabase_TestModel4_365Days, targetOutputVariable_TestModel4_365Days)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "#The n_trials argument specifies the number of iterations for the hyperparameter optimization process. \n",
    "#If you increase n_trials, Optuna will search through a larger portion of the hyperparameter space and potentially find a better set of hyperparameters, but it will take longer because more models need to be trained and evaluated.\n",
    "#If you decrease n_trials, the process will be faster, but Optuna might not find the optimal set of hyperparameters because it has fewer opportunities to explore the hyperparameter space.\n",
    "study.optimize(lambda trial: objective(trial, lgb_ToTrainModel4_365Days, lgb_evalualtePerformanceDuringTrainingModel4_365Days, inputFeatureDatabase_TestModel4_365Days, targetOutputVariable_TestModel4_365Days), n_trials=100)\n",
    "\n",
    "best_paramsForModel4 = study.best_params\n",
    "print(\"Best Parameters:\", best_paramsForModel4)\n",
    "\n",
    "# Re-run the model with the best parameters\n",
    "best_modelForModel4 = buildModel(best_paramsForModel4, lgb_ToTrainModel4_365Days, lgb_evalualtePerformanceDuringTrainingModel4_365Days, 30)\n",
    "predicted_outputForModel4 = prediction(best_modelForModel4, inputFeatureDatabase_TestModel4_365Days)\n",
    "\n",
    "# Calculate the error with the best model\n",
    "best_mseForModel4, best_rmseForModel4 = calculateMSERmse(targetOutputVariable_TestModel4_365Days, predicted_outputForModel4)\n",
    "\n",
    "# Print intermediate results\n",
    "print(\"Best MSE:\", best_mseForModel4)\n",
    "print(\"Best RMSE:\", best_rmseForModel4)\n",
    "\n",
    "#save the model \n",
    "filename = \"model4.txt\"\n",
    "pickle.dump(best_modelForModel3, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes a prediction on the test set using a trained model. The predicted values are saved to y_pred.\n",
    "It calculates the mean squared error (MSE) and root mean squared error (RMSE) between the true target values (y_test) and the predicted values (y_pred).\n",
    "It prints the values of MSE and RMSE to the console.\n",
    "In simple terms, the code is checking how well the trained model is performing on the test set. The MSE and RMSE are measures of how close the predicted values are to the true values. A lower value of MSE or RMSE indicates a better performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing in a plot\n",
    "def plottingTheDifferenceBetweenPredictionAndActualValues(targetOutputVariable_Test, predictedOutputValues):\n",
    "    x_ax = range(len(targetOutputVariable_Test))\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    plt.plot(x_ax, targetOutputVariable_Test, label=\"original\")\n",
    "    plt.plot(x_ax, predictedOutputValues, label=\"predicted\")\n",
    "    plt.title(\"Prediction of Closing Price After 30 Days\")\n",
    "    \n",
    "    plt.ylabel('Price')\n",
    "    plt.legend(loc='best',fancybox=True, shadow=True)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plottingTheDifferenceBetweenPredictionAndActualValues(targetOutputVariable_TestModel1_30Days, predicted_outputForModel1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting feature importance\n",
    "def featureImportancePlot(model,title):\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    lgb.plot_importance(model, ax=ax, height=.70)\n",
    "    ax.set_title(title)\n",
    "    # adjust the spacing between the y-axis and the left edge of the plot\n",
    "    plt.subplots_adjust(left=0.3)\n",
    "    plt.show()\n",
    "\n",
    "featureImportancePlot(best_modelForModel1,'Feature Importance for 30 Days Prediction')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model and testing on recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splittingTheDatasetOfTheUpdatedInfo(database,nameOfPredictedColumn,predictionDays):\n",
    "    database = database.iloc[:-predictionDays]\n",
    "    print(database)\n",
    "    inputFeatureDatabase = database[['Open','High','Low','Close','Volume','Dividends','Stock Splits','Price Change','Price Volatility','SMA','EMA','ADX5','ADX10','ADX20',\n",
    "                                     'ATR5','ATR10','RSI', 'Upper_BBand5','Mid_BBand5','Lower_BBand5','Upper_BBand10','Mid_BBand10','Lower_BBand10','MACD','macd_signal','macd_hist',\n",
    "                                     'OBV','MFI']]\n",
    "\n",
    "    targetOutputVariable = database[[nameOfPredictedColumn]]\n",
    "\n",
    "    return inputFeatureDatabase, targetOutputVariable\n",
    "\n",
    "def gettingRecentPredictions(filename,file_path1,predictionDays,predictDays,windowValue):\n",
    "\n",
    "    #to load the model\n",
    "    loadedModel1 = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "    #to predict the output\n",
    "\n",
    "    #Reading all CSV files in a list\n",
    "    df = pd.read_csv(file_path1)\n",
    "    #print(appleDb.to_string()) - to view the original dataframe\n",
    "\n",
    "    # Convert Date column to datetime object\n",
    "    df['Date'] = pd.to_datetime(df['Date'], utc= True) #set the utc parameter to True to ensure that the timezone information is preserved\n",
    "\n",
    "    # Filter rows based on condition - starting from the year 2008\n",
    "    df = df[pd.to_datetime(df['Date']).dt.year >= 2008]\n",
    "    df = changeValuesIntoLogs(df)\n",
    "    df = obtainingExtraFeatures(df,windowValue)\n",
    "    df = obtainRecentRecords(df)\n",
    "    lastestInput, targetOutput = splittingTheDatasetOfTheUpdatedInfo(df,predictionDays,predictDays)\n",
    "\n",
    "    # Create the LGBM Dataset\n",
    "    lgb_dataset = lgb.Dataset(lastestInput, label=targetOutput)\n",
    "    predictions = loadedModel1.predict(lastestInput)\n",
    "    # Set NumPy print options to display the entire array\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "    # Print the predictions\n",
    "    print(predictions)\n",
    "    return predictions,df\n",
    "\n",
    "def determineWorthOfInvestment(df,predictions,actualClosePrice,differencePrice,whenToPredict):\n",
    "    print(df)\n",
    "    # Get the current close price of the day you want to predict\n",
    "    current_close_price = df['Close'].iloc[-whenToPredict]\n",
    "    current_After_closePrice = df[actualClosePrice].iloc[-whenToPredict]\n",
    "    differencePriceAmount = df[differencePrice].iloc[-whenToPredict]\n",
    "    print(\"Date:\",df['Date'].iloc[-whenToPredict] )\n",
    "    print(\"Price:\", current_close_price)\n",
    "    print(\"After Price:\", current_After_closePrice)\n",
    "    print(\"Actual Diff:\", differencePriceAmount)\n",
    "    # Set the investment threshold (you can adjust this based on your strategy)\n",
    "\n",
    "    print(\"Threshold:\", 0.02)\n",
    "    print(\"Predicted Diff :\", predictions[-whenToPredict])\n",
    "\n",
    "    return predictions[-whenToPredict]\n",
    "\n",
    "    \n",
    "def getPredictionValue(predictionValue):\n",
    "    if predictionValue > 0.02:\n",
    "        return \"BUY\"\n",
    "    elif predictionValue <-0.02:\n",
    "        return \"SELL\"\n",
    "    else:\n",
    "        return \"HOLD\"\n",
    "\n",
    "\n",
    "predictions30Days, latestDataset = gettingRecentPredictions(\"./model1.txt\",'./SingleExampleUpdated/AAPL.csv','DifferenceAfter30Days',30,10)\n",
    "predictions60Days, latestDataset = gettingRecentPredictions(\"./model2.txt\",'./SingleExampleUpdated/AAPL.csv','DifferenceAfter60Days',60,10)\n",
    "predictions120Days, latestDataset = gettingRecentPredictions(\"./model3.txt\",'./SingleExampleUpdated/AAPL.csv','DifferenceAfter120Days',120,10)\n",
    "predictions365Days, latestDataset = gettingRecentPredictions(\"./model4.txt\",'./SingleExampleUpdated/AAPL.csv','DifferenceAfter365Days',365,10)\n",
    "print(latestDataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stating whether to invest or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worth30DayModel = determineWorthOfInvestment(latestDataset,predictions30Days,'ClosePriceAfter30Days','DifferenceAfter30Days',1)\n",
    "worth60DayModel =determineWorthOfInvestment(latestDataset,predictions60Days,'ClosePriceAfter60Days','DifferenceAfter60Days',1)\n",
    "worth120DayModel =determineWorthOfInvestment(latestDataset,predictions120Days,'ClosePriceAfter120Days','DifferenceAfter120Days',1)\n",
    "worth365DayModel =determineWorthOfInvestment(latestDataset,predictions365Days,'ClosePriceAfter365Days','DifferenceAfter365Days',1)\n",
    "\n",
    "\n",
    "predictedValue = getPredictionValue(worth30DayModel)\n",
    "print(\"Investing as predicted value is: \", predictedValue)\n",
    "\n",
    "\n",
    "predictedValue = getPredictionValue(worth60DayModel)\n",
    "print(\"Investing as predicted value is: \", predictedValue)\n",
    "predictedValue = getPredictionValue(worth120DayModel)\n",
    "print(\"Investing as predicted value is: \", predictedValue)\n",
    "predictedValue = getPredictionValue(worth365DayModel)\n",
    "print(\"Investing as predicted value is: \", predictedValue)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing multiple dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path where your files are located\n",
    "directory = './TrainingDatasets'\n",
    "\n",
    "# Iterate over the file list and perform operations on each file\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)  # Create the full file path\n",
    "    \n",
    "    # Check if the file is a CSV file (you can modify the condition as per your file format)\n",
    "    if filename.endswith('.csv'):\n",
    "        df_name = os.path.splitext(filename)[0]  # Get the name of the dataframe\n",
    "        \n",
    "        # Load the dataframe from the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Convert Date column to datetime object\n",
    "        df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "\n",
    "        # Filter rows based on condition - starting from the year 2008\n",
    "        df = df[pd.to_datetime(df['Date']).dt.year >= 2008]\n",
    "        \n",
    "        df = changeValuesIntoLogs(df)\n",
    "        df = obtainingExtraFeatures(df, 10)\n",
    "        df = obtainRecentRecords(appleDb)\n",
    "        \n",
    "        # -------------------------------------------------------------------Model for 30 Day Prediction ------------------------------------------------------------------------\n",
    "\n",
    "        # Prepare the training and testing data for model 1 - 30 days\n",
    "        inputFeatureDatabase_TrainModel1_30Days, inputFeatureDatabase_TestModel1_30Days, targetOutputVariable_TrainModel1_30Days, targetOutputVariable_TestModel1_30Days  = splittingTheDataset(appleDb, 'DifferenceAfter30Days', 30, 0.2, 42)\n",
    "\n",
    "        lgb_ToTrainModel1_30Days, lgb_evalualtePerformanceDuringTrainingModel1_30Days = changingIntoLightGBMformat(inputFeatureDatabase_TrainModel1_30Days, targetOutputVariable_TrainModel1_30Days, inputFeatureDatabase_TestModel1_30Days, targetOutputVariable_TestModel1_30Days)\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective(trial, lgb_ToTrainModel1_30Days, lgb_evalualtePerformanceDuringTrainingModel1_30Days, inputFeatureDatabase_TestModel1_30Days, targetOutputVariable_TestModel1_30Days), n_trials=30, show_progress_bar=False)\n",
    "\n",
    "        best_paramsForModel1 = study.best_params\n",
    "\n",
    "        # Re-run the model with the best parameters\n",
    "        best_modelForModel1 = buildModel(best_paramsForModel1, lgb_ToTrainModel1_30Days, lgb_evalualtePerformanceDuringTrainingModel1_30Days, 30) \n",
    "        predicted_outputForModel1 = prediction(best_modelForModel1, inputFeatureDatabase_TestModel1_30Days)\n",
    "\n",
    "        # Calculate the error with the best model\n",
    "        best_mseForModel1, best_rmseForModel1 = calculateMSERmse(targetOutputVariable_TestModel1_30Days, predicted_outputForModel1)\n",
    "\n",
    "        # save the model \n",
    "        filename = df_name + \"model1.txt\"\n",
    "        pickle.dump(best_modelForModel1, open(filename, 'wb'))\n",
    "\n",
    "        # At the end of each loop iteration, before starting the next file:\n",
    "        del df, lgb_ToTrainModel1_30Days, lgb_evalualtePerformanceDuringTrainingModel1_30Days\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path where your files are located\n",
    "directory = './TrainingDatasets'\n",
    "\n",
    "# Iterate over the file list and perform operations on each file\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)  # Create the full file path\n",
    "    \n",
    "    # Check if the file is a CSV file (you can modify the condition as per your file format)\n",
    "    if filename.endswith('.csv'):\n",
    "        df_name = os.path.splitext(filename)[0]  # Get the name of the dataframe\n",
    "        \n",
    "        # Load the dataframe from the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Convert Date column to datetime object\n",
    "        df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "\n",
    "        # Filter rows based on condition - starting from the year 2008\n",
    "        df = df[pd.to_datetime(df['Date']).dt.year >= 2008]\n",
    "        \n",
    "        df = changeValuesIntoLogs(df)\n",
    "        df = obtainingExtraFeatures(df, 10)\n",
    "        df = obtainRecentRecords(appleDb)\n",
    "        \n",
    "\n",
    "        # ----------------------------------------------------------------------------60 day model--------------------------------------------------------------------------------- \n",
    "\n",
    "        # Prepare the training and testing data for model 2 - 60 days\n",
    "        inputFeatureDatabase_TrainModel2_60Days, inputFeatureDatabase_TestModel2_60Days, targetOutputVariable_TrainModel2_60Days, targetOutputVariable_TestModel2_60Days  = splittingTheDataset(appleDb, 'DifferenceAfter60Days', 60, 0.2, 42)\n",
    "\n",
    "        lgb_ToTrainModel2_60Days, lgb_evalualtePerformanceDuringTrainingModel2_60Days = changingIntoLightGBMformat(inputFeatureDatabase_TrainModel2_60Days, targetOutputVariable_TrainModel2_60Days, inputFeatureDatabase_TestModel2_60Days, targetOutputVariable_TestModel2_60Days)\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective(trial, lgb_ToTrainModel2_60Days, lgb_evalualtePerformanceDuringTrainingModel2_60Days, inputFeatureDatabase_TestModel2_60Days, targetOutputVariable_TestModel2_60Days), n_trials=30, show_progress_bar=False)\n",
    "\n",
    "        best_paramsForModel2 = study.best_params\n",
    "\n",
    "        # Re-run the model with the best parameters\n",
    "        best_modelForModel2 = buildModel(best_paramsForModel2, lgb_ToTrainModel2_60Days, lgb_evalualtePerformanceDuringTrainingModel2_60Days, 30)\n",
    "        predicted_outputForModel2 = prediction(best_modelForModel2, inputFeatureDatabase_TestModel2_60Days)\n",
    "\n",
    "        # Calculate the error with the best model\n",
    "        best_mseForModel2, best_rmseForModel2 = calculateMSERmse(targetOutputVariable_TestModel2_60Days, predicted_outputForModel2)\n",
    "\n",
    "        # save the model \n",
    "        filename = df_name + \"model2.txt\"\n",
    "        pickle.dump(best_modelForModel2, open(filename, 'wb'))\n",
    "\n",
    "        \n",
    "        # At the end of each loop iteration, before starting the next file:\n",
    "        del df, lgb_ToTrainModel2_60Days, lgb_evalualtePerformanceDuringTrainingModel2_60Days\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path where your files are located\n",
    "directory = './TrainingDatasets'\n",
    "\n",
    "# Iterate over the file list and perform operations on each file\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)  # Create the full file path\n",
    "    \n",
    "    # Check if the file is a CSV file (you can modify the condition as per your file format)\n",
    "    if filename.endswith('.csv'):\n",
    "        df_name = os.path.splitext(filename)[0]  # Get the name of the dataframe\n",
    "        \n",
    "        # Load the dataframe from the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Convert Date column to datetime object\n",
    "        df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "\n",
    "        # Filter rows based on condition - starting from the year 2008\n",
    "        df = df[pd.to_datetime(df['Date']).dt.year >= 2008]\n",
    "        \n",
    "        df = changeValuesIntoLogs(df)\n",
    "        df = obtainingExtraFeatures(df, 10)\n",
    "        df = obtainRecentRecords(appleDb)\n",
    "        \n",
    "\n",
    "        # -------------------------------------------------------------------------120 day model---------------------------------------------------------------------------------------\n",
    "\n",
    "        # Prepare the training and testing data for model 3 - 120 days\n",
    "        inputFeatureDatabase_TrainModel3_120Days, inputFeatureDatabase_TestModel3_120Days, targetOutputVariable_TrainModel3_120Days, targetOutputVariable_TestModel3_120Days  = splittingTheDataset(appleDb, 'DifferenceAfter120Days', 120, 0.2, 42)\n",
    "\n",
    "        lgb_ToTrainModel3_120Days, lgb_evalualtePerformanceDuringTrainingModel3_120Days = changingIntoLightGBMformat(inputFeatureDatabase_TrainModel3_120Days, targetOutputVariable_TrainModel3_120Days, inputFeatureDatabase_TestModel3_120Days, targetOutputVariable_TestModel3_120Days)\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective(trial, lgb_ToTrainModel3_120Days, lgb_evalualtePerformanceDuringTrainingModel3_120Days, inputFeatureDatabase_TestModel3_120Days, targetOutputVariable_TestModel3_120Days), n_trials=30, show_progress_bar=False)\n",
    "\n",
    "        best_paramsForModel3 = study.best_params\n",
    "\n",
    "        # Re-run the model with the best parameters\n",
    "        best_modelForModel3 = buildModel(best_paramsForModel3, lgb_ToTrainModel3_120Days, lgb_evalualtePerformanceDuringTrainingModel3_120Days, 30)\n",
    "        predicted_outputForModel3 = prediction(best_modelForModel3, inputFeatureDatabase_TestModel3_120Days)\n",
    "\n",
    "        # Calculate the error with the best model\n",
    "        best_mseForModel3, best_rmseForModel3 = calculateMSERmse(targetOutputVariable_TestModel3_120Days, predicted_outputForModel3)\n",
    "\n",
    "        # save the model \n",
    "        filename = df_name + \"model3.txt\"\n",
    "        pickle.dump(best_modelForModel3, open(filename, 'wb'))\n",
    "\n",
    "        \n",
    "        # At the end of each loop iteration, before starting the next file:\n",
    "        del df, lgb_ToTrainModel3_120Days, lgb_evalualtePerformanceDuringTrainingModel3_120Days\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path where your files are located\n",
    "directory = './TrainingDatasets'\n",
    "\n",
    "# Iterate over the file list and perform operations on each file\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)  # Create the full file path\n",
    "    \n",
    "    # Check if the file is a CSV file (you can modify the condition as per your file format)\n",
    "    if filename.endswith('.csv'):\n",
    "        df_name = os.path.splitext(filename)[0]  # Get the name of the dataframe\n",
    "        \n",
    "        # Load the dataframe from the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Convert Date column to datetime object\n",
    "        df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "\n",
    "        # Filter rows based on condition - starting from the year 2008\n",
    "        df = df[pd.to_datetime(df['Date']).dt.year >= 2008]\n",
    "        \n",
    "        df = changeValuesIntoLogs(df)\n",
    "        df = obtainingExtraFeatures(df, 10)\n",
    "        df = obtainRecentRecords(appleDb)\n",
    "        \n",
    "\n",
    "        # -------------------------------------------------------------------365 day model----------------------------------------------------------------------------------------\n",
    "\n",
    "        # Prepare the training and testing data for model 4 - 365 days\n",
    "        inputFeatureDatabase_TrainModel4_365Days, inputFeatureDatabase_TestModel4_365Days, targetOutputVariable_TrainModel4_365Days, targetOutputVariable_TestModel4_365Days  = splittingTheDataset(appleDb, 'DifferenceAfter365Days', 365, 0.2, 42)\n",
    "\n",
    "        lgb_ToTrainModel4_365Days, lgb_evalualtePerformanceDuringTrainingModel4_365Days = changingIntoLightGBMformat(inputFeatureDatabase_TrainModel4_365Days, targetOutputVariable_TrainModel4_365Days, inputFeatureDatabase_TestModel4_365Days, targetOutputVariable_TestModel4_365Days)\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective(trial, lgb_ToTrainModel4_365Days, lgb_evalualtePerformanceDuringTrainingModel4_365Days, inputFeatureDatabase_TestModel4_365Days, targetOutputVariable_TestModel4_365Days), n_trials=30, show_progress_bar=False)\n",
    "\n",
    "        best_paramsForModel4 = study.best_params\n",
    "\n",
    "        # Re-run the model with the best parameters\n",
    "        best_modelForModel4 = buildModel(best_paramsForModel4, lgb_ToTrainModel4_365Days, lgb_evalualtePerformanceDuringTrainingModel4_365Days, 30)\n",
    "        predicted_outputForModel4 = prediction(best_modelForModel4, inputFeatureDatabase_TestModel4_365Days)\n",
    "\n",
    "        # Calculate the error with the best model\n",
    "        best_mseForModel4, best_rmseForModel4 = calculateMSERmse(targetOutputVariable_TestModel4_365Days, predicted_outputForModel4)\n",
    "\n",
    "        # save the model \n",
    "        filename = df_name + \"model4.txt\"\n",
    "        pickle.dump(best_mseForModel4, open(filename, 'wb'))\n",
    "\n",
    "        \n",
    "        # At the end of each loop iteration, before starting the next file:\n",
    "        del df, lgb_ToTrainModel4_365Days, lgb_evalualtePerformanceDuringTrainingModel4_365Days\n",
    "        gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path where your files are located\n",
    "directory = './UpdatedDatasets'\n",
    "#to keep track of which is the best stock for each model\n",
    "worth30DayModel_dict = {}\n",
    "worth60DayModel_dict = {}\n",
    "worth120DayModel_dict = {}\n",
    "worth365DayModel_dict = {}\n",
    "\n",
    "# Iterate over the file list and perform operations on each file\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)  # Create the full file path\n",
    "    \n",
    "    # Check if the file is a CSV file (you can modify the condition as per your file format)\n",
    "    if filename.endswith('.csv'):\n",
    "        df_name = os.path.splitext(filename)[0]  # Get the name of the dataframe\n",
    "        \n",
    "        # Load the dataframe from the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        #modelName Depending on the dataframe being read\n",
    "        model1 = \"./\" + df_name + \"model1.pkl\"\n",
    "        model2 = \"./\" +  df_name + \"model2.pkl\"\n",
    "        model3 = \"./\" + df_name + \"model3.pkl\"\n",
    "        model4 = \"./\" + df_name + \"model4.pkl\"\n",
    "\n",
    "        #get the latest data and compute the extra features -T.I\n",
    "        predictions30Days, latestDataset = gettingRecentPredictions(model1,file_path,'DifferenceAfter30Days',30,10)\n",
    "        predictions60Days, latestDataset = gettingRecentPredictions(model2,file_path,'DifferenceAfter60Days',60,10)\n",
    "        predictions120Days, latestDataset = gettingRecentPredictions(model3,file_path,'DifferenceAfter120Days',120,10)\n",
    "        predictions365Days, latestDataset = gettingRecentPredictions(model4,file_path,'DifferenceAfter365Days',365,10)\n",
    "\n",
    "        worth30DayModel_dict[df_name] = determineWorthOfInvestment(latestDataset,predictions30Days,'DifferenceAfter30Days',1)\n",
    "        worth60DayModel_dict[df_name] = determineWorthOfInvestment(latestDataset,predictions60Days,'DifferenceAfter60Days',1)\n",
    "        worth120DayModel_dict[df_name] = determineWorthOfInvestment(latestDataset,predictions120Days,'DifferenceAfter120Days',1)\n",
    "        worth365DayModel_dict[df_name] = determineWorthOfInvestment(latestDataset,predictions365Days,'DifferenceAfter365Days',1)\n",
    "\n",
    "\n",
    "        print(df.head())\n",
    "\n",
    "best_df_30DayModel = max(worth30DayModel_dict, key=worth30DayModel_dict.get)\n",
    "best_df_60DayModel = max(worth60DayModel_dict, key=worth60DayModel_dict.get)\n",
    "best_df_120DayModel = max(worth120DayModel_dict, key=worth120DayModel_dict.get)\n",
    "best_df_365DayModel = max(worth365DayModel_dict, key=worth365DayModel_dict.get)\n",
    "\n",
    "print(\"30 Day Model: Best DF is \", best_df_30DayModel, \" with value \", worth30DayModel_dict[best_df_30DayModel])\n",
    "print(\"60 Day Model: Best DF is \", best_df_60DayModel, \" with value \", worth60DayModel_dict[best_df_60DayModel])\n",
    "print(\"120 Day Model: Best DF is \", best_df_120DayModel, \" with value \", worth120DayModel_dict[best_df_120DayModel])\n",
    "print(\"365 Day Model: Best DF is \", best_df_365DayModel, \" with value \", worth365DayModel_dict[best_df_365DayModel])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IAPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
